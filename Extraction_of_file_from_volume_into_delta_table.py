# Databricks notebook source
# DBTITLE 1,Install all required Libs
# MAGIC %pip install Pdfplumber
# MAGIC %pip install python-docx
# MAGIC dbutils.library.restartPython() 

# COMMAND ----------

# DBTITLE 1,Configure common variables
# MAGIC %run ./Reusable_Variable_Config_CarModel

# COMMAND ----------

print(PDF_PATH)

# COMMAND ----------

# DBTITLE 1,Create Volume if not exists
 s_cr_vol=f"CREATE EXTERNAL VOLUME IF NOT EXISTS {EXTERNAL_VOLUME_FULL_NAME} COMMENT 'This is my external volume to keep all docs' LOCATION '{EXTERNAL_VOLUME_PATH}'"
print(s_cr_vol)

spark.sql(s_cr_vol)

# COMMAND ----------

# DBTITLE 1,Creation of table where document will be uploaded into chunks
 s_cr_table=f"CREATE TABLE IF NOT EXISTS {FULL_TABLE_NAME_FOR_FILE_LOAD} (id BIGINT GENERATED BY DEFAULT AS IDENTITY,rname STRING,text STRING)TBLPROPERTIES(delta.enableChangeDataFeed=True)"
print(s_cr_table)

spark.sql(s_cr_table)

# COMMAND ----------

# MAGIC %sql 
# MAGIC CREATE TABLE IF NOT EXISTS genai.vector_db.LLD(
# MAGIC     id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC     rname STRING,
# MAGIC     text STRING
# MAGIC )TBLPROPERTIES(delta.enableChangeDataFeed=True)
# MAGIC

# COMMAND ----------

#from pyspark.sql import SparkSession

# Create a SparkSession
#spark = SparkSession.builder.getOrCreate()

# COMMAND ----------

import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import ArrayType,StringType
import pandas as pd
from docx import Document
import os
import docx

length_function=len

#pdf_path = '/Volumes/genai/vector_db/SandipExternalVolumeLLD'
pdf_path = PDF_PATH

listoffiles=dbutils.fs.ls(pdf_path)

# Define the SQL statement with a parameter placeholder

sql_query_trunc=f"truncate table {FULL_TABLE_NAME_FOR_FILE_LOAD} "
spark.sql(sql_query_trunc)

for f in listoffiles:
    all_text = ''
    file_path_with_fname=pdf_path+'/'+f.name
    print(file_path_with_fname)  
    file_extension = os.path.splitext(file_path_with_fname)
    #print(file_extension)
    print(file_extension[1])
    if file_extension[1] =='.pdf' :
        print('This is a pdf file')
        with pdfplumber.open(file_path_with_fname) as pdf:
            for pdf_page in pdf.pages:
                single_page_text = pdf_page.extract_text()
                all_text = all_text+'\n'+single_page_text
                #print(all_text)
    else :
        print('This is a doc file')
        doc = docx.Document(file_path_with_fname)
        for para in doc.paragraphs:
            all_text = all_text+'\n'+para.text

    splitter=RecursiveCharacterTextSplitter(
        separators=["\n\n","\n"," ",""],
        chunk_size=1000,
        chunk_overlap=200,
        length_function=length_function
        )
    chunks_new=splitter.split_text(all_text)
    print('chunks_done')

    # Let's create a user-defined function (UDF) to chunk all our documents with spark
    @pandas_udf("array<string>")
    def get_chunks(dummy):
        return pd.Series([chunks_new])


    spark.udf.register('get_chunks_udf',get_chunks)

    # Define the SQL statement with a parameter placeholder
    sql_query_insert = f" INSERT INTO {FULL_TABLE_NAME_FOR_FILE_LOAD} (rname, text) SELECT '{f.name}' AS rname, explode(get_chunks_udf('dummy')) AS text "

    print(sql_query_insert)

    # Execute the SQL statement
    spark.sql(sql_query_insert).show

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from genai.vector_db.carmodel
# MAGIC --truncate table  genai.vector_db.carmodel
# MAGIC --delete from genai.vector_db.carmodel where rname='Sample_Code_Reference.docx'
# MAGIC